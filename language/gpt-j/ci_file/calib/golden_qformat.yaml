# --weight_calib_method AMAX_SYM --weight_dtype int8 --act_calib_method MINMAX_ASYM --act_dtype int8 --weight_nbits 8 --act_nbits 8 --weight_granularity channel --act_granularity channel --kv_dtype int8 --kv_granularity head --is_dynamic_quant False --disable_input True --disable_output False
major_dtype:
  weight_dtype: int8
  act_dtype: int8
  weight_granularity: channel
  act_granularity: channel
  kv_dtype: int8
  kv_granularity: head
  disable_input: true
  disable_output: false
quantized op list:
  transformer_wte:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.wte._input_quantizer
        input_dtype: torch.int32
        input_shape:
        - 1
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_weight:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.wte._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 50401
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  transformer_h_0_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_0_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_0_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_0_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_0_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_0_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_0_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_1_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_1_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_1_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_1_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_1_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_1_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_1_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_2_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_2_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_2_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_2_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_2_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_2_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_2_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_3_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_3_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_3_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_3_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_3_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_3_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_3_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_4_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_4_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_4_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_4_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_4_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_4_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_4_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_5_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_5_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_5_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_5_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_5_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_5_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_5_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_6_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_6_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_6_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_6_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_6_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_6_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_6_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_7_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_7_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_7_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_7_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_7_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_7_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_7_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_8_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_8_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_8_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_8_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_8_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_8_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_8_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_9_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_9_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_9_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_9_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_9_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_9_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_9_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_10_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_10_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_10_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_10_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_10_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_10_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_10_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_11_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_11_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_11_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_11_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_11_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_11_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_11_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_12_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_12_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_12_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_12_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_12_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_12_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_12_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_13_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_13_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_13_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_13_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_13_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_13_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_13_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_14_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_14_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_14_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_14_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_14_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_14_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_14_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_15_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_15_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_15_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_15_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_15_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_15_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_15_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_16_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_16_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_16_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_16_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_16_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_16_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_16_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_17_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_17_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_17_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_17_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_17_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_17_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_17_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_18_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_18_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_18_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_18_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_18_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_18_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_18_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_19_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_19_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_19_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_19_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_19_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_19_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_19_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_20_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_20_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_20_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_20_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_20_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_20_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_20_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_21_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_21_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_21_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_21_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_21_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_21_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_21_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_22_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_22_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_22_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_22_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_22_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_22_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_22_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_23_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_23_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_23_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_23_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_23_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_23_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_23_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_24_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_24_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_24_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_24_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_24_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_24_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_24_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_25_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_25_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_25_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_25_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_25_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_25_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_25_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_26_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_26_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_26_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_26_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_26_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_26_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_26_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_27_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_27_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_27_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_27_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_27_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_27_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_27_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_ln_f:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.ln_f._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  lm_head:
    output_shape:
    - 1
    - 2048
    - 50401
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: lm_head._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: lm_head._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 50401
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  sub:
    output_shape:
    - 1
    - 1
    - 1
    - 2048
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: sub._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: sub._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul:
    output_shape:
    - 1
    - 1
    - 1
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  einsum:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_1:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_1._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_1._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_1:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_1._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_1._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_4:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_4._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_4._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_1:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_1._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_1._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_1:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_1._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_1._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_1:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_1._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_1._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_2:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_2._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_2._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_6:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_6._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_6._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_3:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_3._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_3._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_7:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_7._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_7._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_8:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_8._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_8._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_2:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_2._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_2._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_3:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_3._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_3._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_2:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_2._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_2._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_3:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_3._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_3._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_2:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_2._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_2._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_2:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_2._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_2._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_12:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_12._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_12._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_1:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_3:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_3._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_3._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_4:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_4._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_4._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_3:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_3._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_3._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_1:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_5:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_5._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_5._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_14:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_14._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_14._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_6:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_6._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_6._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_15:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_15._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_15._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_16:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_16._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_16._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_4:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_4._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_4._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_5:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_5._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_5._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_4:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_4._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_4._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_5:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_5._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_5._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_4:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_4._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_4._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_4:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_4._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_4._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_20:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_20._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_20._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_2:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_2._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_5:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_5._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_5._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_7:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_7._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_7._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_5:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_5._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_5._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_2:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_2._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_8:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_8._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_8._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_22:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_22._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_22._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_9:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_9._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_9._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_23:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_23._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_23._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_24:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_24._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_24._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_6:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_6._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_6._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_7:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_7._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_7._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_6:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_6._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_6._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_7:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_7._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_7._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_6:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_6._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_6._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_6:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_6._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_6._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_28:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_28._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_28._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_3:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_3._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_7:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_7._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_7._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_10:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_10._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_10._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_7:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_7._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_7._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_3:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_3._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_11:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_11._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_11._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_30:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_30._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_30._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_12:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_12._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_12._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_31:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_31._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_31._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_32:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_32._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_32._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_8:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_8._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_8._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_9:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_9._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_9._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_8:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_8._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_8._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_9:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_9._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_9._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_8:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_8._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_8._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_8:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_8._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_8._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_36:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_36._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_36._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_4:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_4._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_9:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_9._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_9._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_13:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_13._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_13._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_9:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_9._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_9._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_4:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_4._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_14:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_14._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_14._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_38:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_38._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_38._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_15:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_15._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_15._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_39:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_39._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_39._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_40:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_40._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_40._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_10:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_10._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_10._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_11:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_11._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_11._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_10:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_10._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_10._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_11:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_11._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_11._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_10:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_10._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_10._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_10:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_10._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_10._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_44:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_44._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_44._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_5:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_5._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_11:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_11._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_11._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_16:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_16._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_16._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_11:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_11._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_11._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_5:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_5._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_17:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_17._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_17._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_46:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_46._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_46._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_18:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_18._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_18._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_47:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_47._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_47._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_48:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_48._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_48._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_12:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_12._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_12._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_13:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_13._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_13._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_12:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_12._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_12._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_13:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_13._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_13._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_12:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_12._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_12._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_12:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_12._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_12._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_52:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_52._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_52._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_6:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_6._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_13:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_13._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_13._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_19:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_19._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_19._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_13:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_13._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_13._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_6:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_6._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_20:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_20._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_20._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_54:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_54._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_54._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_21:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_21._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_21._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_55:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_55._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_55._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_56:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_56._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_56._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_14:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_14._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_14._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_15:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_15._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_15._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_14:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_14._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_14._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_15:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_15._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_15._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_14:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_14._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_14._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_14:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_14._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_14._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_60:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_60._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_60._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_7:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_7._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_15:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_15._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_15._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_22:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_22._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_22._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_15:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_15._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_15._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_7:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_7._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_23:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_23._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_23._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_62:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_62._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_62._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_24:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_24._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_24._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_63:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_63._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_63._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_64:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_64._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_64._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_16:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_16._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_16._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_17:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_17._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_17._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_16:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_16._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_16._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_17:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_17._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_17._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_16:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_16._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_16._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_16:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_16._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_16._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_68:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_68._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_68._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_8:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_8._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_17:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_17._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_17._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_25:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_25._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_25._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_17:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_17._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_17._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_8:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_8._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_26:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_26._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_26._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_70:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_70._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_70._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_27:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_27._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_27._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_71:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_71._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_71._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_72:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_72._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_72._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_18:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_18._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_18._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_19:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_19._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_19._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_18:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_18._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_18._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_19:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_19._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_19._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_18:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_18._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_18._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_18:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_18._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_18._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_76:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_76._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_76._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_9:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_9._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_19:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_19._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_19._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_28:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_28._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_28._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_19:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_19._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_19._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_9:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_9._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_29:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_29._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_29._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_78:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_78._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_78._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_30:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_30._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_30._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_79:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_79._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_79._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_80:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_80._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_80._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_20:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_20._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_20._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_21:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_21._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_21._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_20:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_20._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_20._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_21:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_21._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_21._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_20:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_20._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_20._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_20:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_20._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_20._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_84:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_84._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_84._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_10:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_10._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_21:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_21._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_21._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_31:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_31._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_31._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_21:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_21._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_21._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_10:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_10._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_32:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_32._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_32._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_86:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_86._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_86._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_33:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_33._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_33._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_87:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_87._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_87._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_88:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_88._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_88._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_22:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_22._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_22._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_23:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_23._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_23._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_22:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_22._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_22._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_23:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_23._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_23._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_22:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_22._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_22._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_22:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_22._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_22._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_92:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_92._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_92._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_11:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_11._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_23:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_23._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_23._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_34:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_34._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_34._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_23:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_23._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_23._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_11:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_11._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_35:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_35._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_35._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_94:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_94._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_94._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_36:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_36._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_36._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_95:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_95._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_95._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_96:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_96._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_96._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_24:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_24._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_24._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_25:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_25._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_25._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_24:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_24._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_24._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_25:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_25._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_25._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_24:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_24._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_24._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_24:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_24._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_24._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_100:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_100._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_100._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_12:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_12._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_25:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_25._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_25._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_37:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_37._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_37._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_25:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_25._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_25._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_12:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_12._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_38:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_38._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_38._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_102:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_102._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_102._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_39:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_39._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_39._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_103:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_103._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_103._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_104:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_104._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_104._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_26:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_26._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_26._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_27:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_27._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_27._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_26:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_26._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_26._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_27:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_27._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_27._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_26:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_26._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_26._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_26:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_26._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_26._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_108:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_108._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_108._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_13:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_13._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_27:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_27._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_27._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_40:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_40._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_40._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_27:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_27._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_27._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_13:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_13._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_41:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_41._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_41._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_110:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_110._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_110._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_42:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_42._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_42._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_111:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_111._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_111._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_112:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_112._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_112._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_28:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_28._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_28._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_29:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_29._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_29._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_28:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_28._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_28._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_29:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_29._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_29._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_28:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_28._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_28._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_28:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_28._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_28._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_116:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_116._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_116._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_14:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_14._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_29:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_29._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_29._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_43:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_43._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_43._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_29:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_29._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_29._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_14:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_14._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_44:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_44._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_44._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_118:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_118._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_118._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_45:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_45._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_45._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_119:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_119._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_119._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_120:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_120._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_120._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_30:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_30._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_30._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_31:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_31._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_31._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_30:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_30._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_30._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_31:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_31._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_31._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_30:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_30._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_30._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_30:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_30._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_30._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_124:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_124._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_124._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_15:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_15._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_31:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_31._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_31._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_46:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_46._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_46._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_31:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_31._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_31._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_15:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_15._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_47:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_47._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_47._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_126:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_126._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_126._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_48:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_48._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_48._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_127:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_127._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_127._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_128:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_128._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_128._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_32:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_32._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_32._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_33:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_33._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_33._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_32:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_32._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_32._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_33:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_33._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_33._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_32:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_32._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_32._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_32:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_32._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_32._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_132:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_132._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_132._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_16:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_16._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_33:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_33._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_33._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_49:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_49._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_49._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_33:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_33._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_33._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_16:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_16._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_50:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_50._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_50._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_134:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_134._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_134._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_51:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_51._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_51._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_135:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_135._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_135._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_136:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_136._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_136._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_34:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_34._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_34._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_35:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_35._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_35._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_34:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_34._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_34._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_35:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_35._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_35._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_34:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_34._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_34._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_34:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_34._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_34._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_140:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_140._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_140._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_17:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_17._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_35:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_35._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_35._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_52:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_52._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_52._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_35:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_35._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_35._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_17:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_17._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_53:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_53._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_53._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_142:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_142._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_142._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_54:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_54._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_54._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_143:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_143._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_143._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_144:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_144._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_144._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_36:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_36._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_36._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_37:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_37._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_37._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_36:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_36._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_36._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_37:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_37._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_37._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_36:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_36._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_36._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_36:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_36._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_36._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_148:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_148._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_148._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_18:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_18._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_37:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_37._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_37._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_55:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_55._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_55._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_37:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_37._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_37._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_18:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_18._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_56:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_56._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_56._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_150:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_150._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_150._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_57:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_57._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_57._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_151:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_151._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_151._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_152:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_152._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_152._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_38:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_38._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_38._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_39:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_39._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_39._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_38:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_38._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_38._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_39:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_39._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_39._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_38:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_38._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_38._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_38:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_38._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_38._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_156:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_156._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_156._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_19:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_19._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_39:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_39._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_39._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_58:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_58._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_58._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_39:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_39._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_39._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_19:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_19._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_59:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_59._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_59._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_158:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_158._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_158._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_60:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_60._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_60._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_159:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_159._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_159._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_160:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_160._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_160._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_40:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_40._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_40._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_41:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_41._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_41._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_40:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_40._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_40._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_41:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_41._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_41._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_40:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_40._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_40._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_40:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_40._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_40._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_164:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_164._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_164._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_20:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_20._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_41:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_41._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_41._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_61:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_61._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_61._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_41:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_41._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_41._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_20:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_20._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_62:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_62._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_62._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_166:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_166._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_166._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_63:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_63._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_63._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_167:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_167._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_167._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_168:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_168._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_168._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_42:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_42._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_42._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_43:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_43._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_43._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_42:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_42._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_42._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_43:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_43._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_43._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_42:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_42._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_42._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_42:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_42._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_42._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_172:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_172._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_172._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_21:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_21._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_43:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_43._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_43._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_64:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_64._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_64._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_43:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_43._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_43._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_21:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_21._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_65:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_65._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_65._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_174:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_174._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_174._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_66:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_66._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_66._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_175:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_175._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_175._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_176:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_176._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_176._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_44:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_44._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_44._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_45:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_45._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_45._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_44:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_44._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_44._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_45:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_45._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_45._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_44:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_44._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_44._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_44:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_44._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_44._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_180:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_180._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_180._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_22:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_22._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_45:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_45._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_45._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_67:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_67._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_67._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_45:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_45._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_45._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_22:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_22._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_68:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_68._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_68._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_182:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_182._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_182._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_69:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_69._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_69._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_183:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_183._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_183._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_184:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_184._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_184._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_46:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_46._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_46._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_47:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_47._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_47._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_46:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_46._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_46._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_47:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_47._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_47._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_46:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_46._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_46._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_46:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_46._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_46._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_188:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_188._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_188._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_23:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_23._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_47:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_47._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_47._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_70:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_70._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_70._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_47:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_47._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_47._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_23:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_23._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_71:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_71._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_71._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_190:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_190._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_190._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_72:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_72._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_72._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_191:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_191._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_191._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_192:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_192._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_192._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_48:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_48._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_48._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_49:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_49._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_49._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_48:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_48._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_48._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_49:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_49._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_49._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_48:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_48._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_48._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_48:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_48._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_48._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_196:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_196._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_196._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_24:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_24._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_49:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_49._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_49._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_73:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_73._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_73._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_49:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_49._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_49._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_24:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_24._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_74:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_74._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_74._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_198:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_198._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_198._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_75:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_75._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_75._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_199:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_199._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_199._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_200:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_200._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_200._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_50:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_50._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_50._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_51:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_51._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_51._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_50:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_50._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_50._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_51:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_51._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_51._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_50:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_50._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_50._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_50:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_50._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_50._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_204:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_204._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_204._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_25:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_25._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_51:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_51._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_51._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_76:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_76._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_76._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_51:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_51._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_51._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_25:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_25._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_77:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_77._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_77._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_206:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_206._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_206._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_78:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_78._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_78._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_207:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_207._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_207._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_208:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_208._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_208._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_52:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_52._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_52._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_53:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_53._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_53._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_52:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_52._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_52._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_53:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_53._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_53._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_52:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_52._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_52._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_52:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_52._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_52._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_212:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_212._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_212._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_26:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_26._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_53:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_53._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_53._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_79:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_79._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_79._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_53:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_53._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_53._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_26:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_26._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_80:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_80._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_80._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_214:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_214._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_214._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_81:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_81._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_81._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_215:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_215._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_215._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_216:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_216._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_216._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_54:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_54._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_54._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_55:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_55._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_55._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_54:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_54._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_54._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_55:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_55._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_55._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_54:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_54._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_54._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_54:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_54._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_54._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_220:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_220._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_220._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 1
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_27:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_27._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_55:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_55._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_55._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_82:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_82._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_82._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_55:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_55._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_55._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_27:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_27._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_83:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_83._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_83._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_222:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_222._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_222._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_84:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_84._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_84._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_223:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_223._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_223._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_224:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_224._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_224._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_0_quantize_node:
    output_shape:
    - 1
    - 2048
    - 50401
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_0_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 50401
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_1_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_1_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_2_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_2_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_3_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_3_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_4_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_4_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_5_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_5_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_6_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_6_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_7_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_7_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_8_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_8_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_9_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_9_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_10_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_10_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_11_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_11_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_12_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_12_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_13_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_13_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_14_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_14_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_15_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_15_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_16_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_16_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_17_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_17_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_18_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_18_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_19_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_19_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_20_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_20_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_21_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_21_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_22_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_22_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_23_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_23_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_24_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_24_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_25_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_25_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_26_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_26_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_27_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_27_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_28_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_28_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_29_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_29_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_30_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_30_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_31_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_31_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_32_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_32_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_33_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_33_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_34_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_34_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_35_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_35_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_36_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_36_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_37_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_37_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_38_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_38_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_39_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_39_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_40_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_40_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_41_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_41_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_42_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_42_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_43_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_43_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_44_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_44_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_45_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_45_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_46_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_46_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_47_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_47_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_48_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_48_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_49_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_49_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_50_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_50_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_51_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_51_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_52_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_52_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_53_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_53_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_54_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_54_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_55_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_55_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_56_quantize_node:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_56_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
